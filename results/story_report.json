{
  "action_reset_cast": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1248
  },
  "utter_self_introduction": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_goodbye": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 624
  },
  "action_recommendation_without_movie": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1244
  },
  "action_reset_release_date": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1252
  },
  "utter_ask_enable_release_date": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1773
  },
  "utter_ask_rating": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 677
  },
  "action_reset_plot": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1246
  },
  "action_reset_genre": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1248
  },
  "utter_ask_enable_genre": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1809
  },
  "action_reset_rating": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1248
  },
  "utter_user_praise": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 624
  },
  "utter_greet": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1
  },
  "utter_decision": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 625
  },
  "utter_ask_movie_period": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1590
  },
  "utter_ask_if_ok": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1845
  },
  "utter_ask_director_name": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 605
  },
  "utter_ask_enable_rating": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1845
  },
  "retrieve_plot_form": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_ask_changes": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 615
  },
  "action_summary_request": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1845
  },
  "utter_ask_to_continue": {
    "precision": 1.0,
    "recall": 0.9983974358974359,
    "f1-score": 0.9991980753809142,
    "support": 1248
  },
  "utter_confirm": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 158
  },
  "utter_ask_genre": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 629
  },
  "utter_ask_enable_director_name": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1773
  },
  "action_reset_slots": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1845
  },
  "utter_ask_enable_cast": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1773
  },
  "utter_ask_release_date": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 789
  },
  "utter_ask_cast": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 605
  },
  "action_reset_director_name": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1248
  },
  "action_recommendation_with_movie": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2
  },
  "utter_return_recommended_movie": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 1246
  },
  "action_listen": {
    "precision": 0.9998937752283833,
    "recall": 1.0,
    "f1-score": 0.9999468847931162,
    "support": 18826
  },
  "accuracy": 0.9999616189141991,
  "macro avg": {
    "precision": 0.9999967810675269,
    "recall": 0.9999514374514376,
    "f1-score": 0.9999740897022433,
    "support": 52109
  },
  "weighted avg": {
    "precision": 0.9999616229912212,
    "recall": 0.9999616189141991,
    "f1-score": 0.99996160454414,
    "support": 52109
  },
  "micro avg": {
    "precision": 0.9999616189141991,
    "recall": 0.9999616189141991,
    "f1-score": 0.9999616189141991,
    "support": 52109
  },
  "conversation_accuracy": {
    "accuracy": 0.9989276139410188,
    "correct": 1863,
    "with_warnings": 0,
    "total": 1865
  }
}